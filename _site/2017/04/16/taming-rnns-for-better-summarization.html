<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Taming Recurrent Neural Networks for Better Summarization | Abigail See</title>
	<meta name="description" content="This is a blog post about our latest paper, Get To The Point: Summarization with Pointer-Generator Networks, to appear at ACL 2017. The code is available here.">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!-- <meta http-equiv="X-Frame-Options" content="sameorigin"> -->

	<!-- CSS -->
	<link rel="stylesheet" href="/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="http://localhost:4000/2017/04/16/taming-rnns-for-better-summarization.html">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Abigail See" href="http://localhost:4000/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	<link href="//fonts.googleapis.com/css?family=Courier:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
	<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.3.0/katex.min.css">
	<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.3.0/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-97611017-1', 'auto');
	ga('send', 'pageview');
	</script>
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<h1 class="site-title">
			<a href="/">Abigail See</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
			
			
			
			
			<li>
				<a class="page-link" href="/about/">
					About
				</a>
			</li>
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled -->
			







































		</ul>
	</nav>
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/')">
    <h1 class="title">Taming Recurrent Neural Networks for Better Summarization</h1>
    
    <p class="meta">
    April 16, 2017
    
    </p>
  </header>
  <section class="post-content"><p><em>This is a blog post about our latest paper, <a href="https://arxiv.org/pdf/1704.04368.pdf">Get To The Point: Summarization with Pointer-Generator Networks</a>, to appear at <a href="http://acl2017.org/">ACL 2017</a>. The code is available <a href="https://github.com/abisee/pointer-generator">here</a>.</em></p>

<p>The internet age has brought unfathomably massive amounts of information to the fingertips of billions – if only we had time to read it.
Though our lives have been transformed by ready access to limitless data, we also find ourselves ensnared by <a href="https://en.wikipedia.org/wiki/Information_overload">information overload</a>.
For this reason, <em>automatic text summarization</em> – the task of automatically condensing a piece of text to a shorter version – is becoming increasingly vital.</p>

<h3 id="two-types-of-summarization">Two types of summarization</h3>

<p>There are broadly two approaches to automatic text summarization: <em>extractive</em> and <em>abstractive</em>.</p>

<ul>
  <li><strong>Extractive</strong> approaches select passages from the source text, then arrange them to form a summary. You might think of these approaches as like a highlighter.</li>
</ul>

<p><img src="http://localhost:4000/img/highlighter.jpg" alt="a highlighter" /></p>

<ul>
  <li><strong>Abstractive</strong> approaches use natural language generation techniques to write novel sentences. By the same analogy, these approaches are like a pen.</li>
</ul>

<p><img src="http://localhost:4000/img/pen.jpg" alt="a pen" /></p>

<p>The great majority of existing approaches to automatic summarization are extractive – mostly because it is much easier to <em>select</em> text than it is to <em>generate</em> text from scratch.
For example, if your extractive approach involves selecting and rearranging whole sentences from the source text, you are guaranteed to produce a summary that is grammatical, fairly readable, and related to the source text.
These systems (several <a href="http://autosummarizer.com">are</a> <a href="http://textcompactor.com/">available</a> <a href="http://smmry.com/">online</a>) can be reasonably successful when applied to mid-length factual text such as news articles and technical documents.</p>

<p>On the other hand, the extractive approach is too restrictive to produce human-like summaries – especially of longer, more complex text.
Imagine trying to write a <a href="https://en.wikipedia.org/wiki/Great_Expectations#Plot_summary">Wikipedia-style plot synopsis</a> of a novel – say, <em>Great Expectations</em> – solely by selecting and rearranging sentences from the book.
This would be impossible.
For one thing, <em>Great Expectations</em> is written in the first person whereas a synopsis should be in the third person.
More importantly, condensing whole chapters of action down to a sentence like <em>Pip visits Miss Havisham and falls in love with her adopted daughter Estella</em> requires powerful paraphrasing that is possible only in an abstractive framework.</p>

<p>In short: abstractive summarization may be difficult, but it’s essential!</p>

<h3 id="enter-recurrent-neural-networks">Enter Recurrent Neural Networks</h3>

<p><em>If you’re unfamiliar with Recurrent Neural Networks or the attention mechanism, check out the excellent tutorials by <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">WildML</a>, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy</a> and <a href="http://distill.pub/2016/augmented-rnns/">Distill</a>.</em></p>

<p>In the past few years, the Recurrent Neural Network (RNN) – a type of neural network that can perform calculations on sequential data (e.g. sequences of words) – has become the standard approach for many Natural Language Processing tasks.
In particular, the <em>sequence-to-sequence model with attention</em>, illustrated below, has become popular for summarization.
Let’s step through the diagram!</p>

<p><img src="http://localhost:4000/img/seq2seq-attn.png" alt="sequence-to-sequence network with attention" /></p>

<p>In this example, our source text is a news article that begins <em>Germany emerge victorious in 2-0 win against Argentina on Saturday</em>, and we’re in the process of producing the abstractive summary <em>Germany beat Argentina 2-0</em>.
First, the <em><font color="#db4437">encoder RNN</font></em> reads in the source text word-by-word, producing a sequence of <em><font color="#db4437">encoder hidden states</font></em>.
(There are arrows in both directions because our encoder is <em>bidirectional</em>, but that’s not important here).</p>

<p>Once the encoder has read the entire source text, the <em><font color="#f4b400">decoder RNN</font></em> begins to output a sequence of words that should form a summary.
On each step, the decoder receives as input the previous word of the summary (on the first step, this is a special &lt;START&gt; token which is the signal to begin writing) and uses it to update the <em><font color="#f4b400">decoder hidden state</font></em>.
This is used to calculate the <em><font color="#4285f4">attention distribution</font></em>, which is a probability distribution over the words in the source text.
Intuitively, the attention distribution tells the network where to look to help it produce the next word.
In the diagram above, the decoder has so far produced the first word <em>Germany</em>, and is concentrating on the source words <em>win</em> and <em>victorious</em> in order to generate the next word <em>beat</em>.</p>

<p>Next, the attention distribution is used to produce a weighted sum of the encoder hidden states, known as the <em><font color="#4285f4">context vector</font></em>.
The context vector can be regarded as “what has been read from the source text” on this step of the decoder.
Finally, the context vector and the decoder hidden state are used to calculate the <em><font color="#0f9d58">vocabulary distribution</font></em>, which is a probability distribution over all the words in a large fixed vocabulary (typically tens or hundreds of thousands of words).
The word with the largest probability (on this step, <em>beat</em>) is chosen as output, and the decoder moves on to the next step.</p>

<p>The decoder’s ability to freely generate words in any order – including words such as <em>beat</em> that do not appear in the source text – makes the sequence-to-sequence model a potentially powerful solution to abstractive summarization.</p>

<h3 id="two-big-problems">Two Big Problems</h3>

<p>Unfortunately, this approach to summarization suffers from two big problems:</p>

<p><strong><u>Problem 1</u></strong>: The summaries sometimes <strong>reproduce factual details inaccurately</strong> (e.g. <em>Germany beat Argentina <strong>3-2</strong></em>). This is especially common for rare or out-of-vocabulary words such as <em>2-0</em>.</p>

<p><strong><u>Problem 2</u></strong>: The summaries sometimes <strong>repeat themselves</strong> (e.g. <em>Germany beat Germany beat Germany beat…</em>)</p>

<p>In fact, these problems are common for RNNs in general.
As always in deep learning, it’s difficult to explain <em>why</em> the network exhibits any particular behavior. For those who are interested, I offer the following conjectures. If you’re not interested, skip ahead to the <a href="#easier-copying-with-pointer-generator-networks">solutions</a>.</p>

<p><strong><u>Explanation for Problem 1</u></strong>: The sequence-to-sequence-with-attention model makes it <em>too difficult</em> to copy a word <em>w</em> from the source text.
The network must somehow recover the original word after the information has passed through several layers of computation (including mapping <em>w</em> to its <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a>).</p>

<p>In particular, if <em>w</em> is a rare word that appeared infrequently during training and therefore has a poor word embedding (i.e. it is clustered with completely unrelated words), then <em>w</em> is, from the perspective of the network, indistinguishable from many other words, thus impossible to reproduce.</p>

<p>Even if <em>w</em> has a good word embedding, the network may still have difficulty reproducing the word.
For example, RNN summarization systems often replace a name with another name (e.g. <em>Anna</em> → <em>Emily</em>) or a city with another city (e.g. <em>Delhi</em> → <em>Mumbai</em>).
This is because the word embeddings for e.g. female names or Indian cities tend to cluster together, which may cause confusion when attempting to reconstruct the original word.</p>

<p>In short, this seems like an unnecessarily difficult way to perform a simple operation – copying – that is a fundamental operation in summarization.</p>

<p><strong><u>Explanation for Problem 2</u></strong>:
Repetition may be caused by the decoder’s <em>over-reliance on the decoder input (i.e. previous summary word)</em>, rather than storing longer-term information in the decoder state.
This can be seen by the fact that a single repeated word commonly triggers an endless repetitive cycle.
For example, a single substitution error <em>Germany beat <strong>Germany</strong></em> leads to the catastrophic <em>Germany beat Germany beat Germany beat…</em>, and not the less-wrong <em>Germany beat Germany 2-0</em>.</p>

<h3 id="easier-copying-with-pointer-generator-networks">Easier Copying with Pointer-Generator Networks</h3>

<p>Our solution for <strong>Problem 1</strong> (inaccurate copying) is the <em>pointer-generator network</em>.
This is a hybrid network that can choose to copy words from the source via <em>pointing</em>, while retaining the ability to <em>generate</em> words from the fixed vocabulary.
Let’s step through the diagram!</p>

<p><img src="http://localhost:4000/img/pointer-gen.png" alt="pointer-generator network" /></p>

<p>This diagram shows the third step of the decoder, when we have so far generated the partial summary <em>Germany beat</em>.
As before, we calculate an <em><font color="#4285f4">attention distribution</font></em> and a <em><font color="#0f9d58">vocabulary distribution</font></em>.
However, we also calculate the <em><font color="#f4b400">generation probability</font></em> <script type="math/tex">p_{\text{gen}}</script>, which is a scalar value between 0 and 1.
This represents the probability of <em>generating</em> a word from the vocabulary, versus <em>copying</em> a word from the source.
The generation probability <script type="math/tex">p_{\text{gen}}</script> is used to weight and combine the <font color="#0f9d58">vocabulary distribution</font> <script type="math/tex">P_{\text{vocab}}</script> (which we use for generating) and the <font color="#4285f4">attention distribution</font> <script type="math/tex">a</script> (which we use for pointing to source words <script type="math/tex">w_i</script>) into the <em>final distribution</em> <script type="math/tex">P_{\text{final}}</script> via the following formula:</p>

<script type="math/tex; mode=display">P_{\text{final}}(w) = p_{\text{gen}} P_{\text{vocab}}(w) + ( 1-p_{\text{gen}} ) \sum_{i: w_i = w} a_i</script>

<p>This formula just says that the probability of producing word <script type="math/tex">w</script> is equal to the probability of generating it from the vocabulary (multiplied by the generation probability) plus the probability of pointing to it anywhere it appears in the source text (multiplied by the copying probability).</p>

<p>Compared to the sequence-to-sequence-with-attention system, the pointer-generator network has several advantages:</p>

<ol>
  <li>The pointer-generator network makes it <strong>easy</strong> to copy words from the source text. The network simply needs to put sufficiently large attention on the relevant word, and make <script type="math/tex">p_{\text{gen}}</script> sufficiently large.</li>
  <li>The pointer-generator model is even able to copy <strong>out-of-vocabulary</strong> words from the source text. This is a major bonus, enabling us to handle unseen words while also allowing us to use a smaller vocabulary (which requires less computation and storage space).</li>
  <li>The pointer-generator model is <strong>faster to train</strong>, requiring fewer training iterations to achieve the same performance as the sequence-to-sequence attention system.</li>
</ol>

<p>In this way, the pointer-generator network is a <em>best of both worlds</em>, combining both extraction (pointing) and abstraction (generating).</p>

<p><img src="http://localhost:4000/img/highlighter_plus_pen_100h.png" alt="highlighter plus pen" /></p>

<h3 id="eliminating-repetition-with-coverage">Eliminating Repetition with Coverage</h3>

<p>To tackle <strong>Problem 2</strong> (repetitive summaries), we use a technique called <em>coverage</em>.
The idea is that we use the attention distribution to keep track of what’s been covered so far, and penalize the network for attending to same parts again.</p>

<p>On each timestep <script type="math/tex">t</script> of the decoder, the <em>coverage vector</em> <script type="math/tex">c^t</script> is the sum of all the attention distributions <script type="math/tex">a^{t'}</script> so far:</p>

<script type="math/tex; mode=display">c^t = \sum_{t'=0}^{t-1}a^{t'}</script>

<p>In other words, the coverage of a particular source word is equal to the amount of attention it has received so far.
In our running example, the coverage vector may build up like so (where yellow shading intensity represents coverage):</p>

<p><img src="http://localhost:4000/img/coverage.gif" alt="example of coverage" /></p>

<p>Lastly, we introduce an extra loss term to penalize any overlap between the coverage vector <script type="math/tex">c^t</script> and the new attention distribution <script type="math/tex">a^t</script>:</p>

<script type="math/tex; mode=display">\text{covloss}_t = \sum_i \min(a_i^t, c_i^t)</script>

<p>This discourages the network from attending to (thus summarizing) anything that’s already been covered.</p>

<h3 id="example-output">Example Output</h3>

<p>Let’s see a comparison of the systems on some real data!
We trained and tested our networks on the <em><a href="http://cs.nyu.edu/~kcho/DMQA/">CNN / Daily Mail</a></em> dataset, which contains news articles paired with multi-sentence summaries.</p>

<p>The example below shows the source text (a news article about rugby) alongside the reference summary that <a href="http://www.dailymail.co.uk/sport/rugbyunion/article-3027560/New-Zealand-international-Francis-Saili-signs-two-year-deal-Munster.html">originally accompanied</a> the article, plus the three automatic summaries produced by our three systems.
By hovering your cursor over a word from one of the automatic summaries, you can view the attention distribution projected in <span style="background-color: #f4e60d">yellow</span> on the source text.
This tells you where the network was “looking” when it produced that word.</p>

<p>For the pointer-generator models, the value of the generation probability is also visualized in <span style="background-color: #16e983">green</span>. Hovering the cursor over a word from one of those summaries will show you the value of the generation probability <script type="math/tex">p_{\text{gen}}</script> for that word.</p>

<p><em>Note: you may need to zoom out using your browser window to view the demo all on one screen. Does not work for mobile.</em></p>

<head>
<script src="http://localhost:4000/js/external/d3.min.js"></script>
<script src="http://localhost:4000/js/external/jquery-3.1.0.min.js"></script>
<script src="http://localhost:4000/js/external/underscore-min.js"></script>
<script src="http://localhost:4000/js/external/sprintf.min.js"></script>
<!-- <link href='//fonts.googleapis.com/css?family=Courier' rel='stylesheet' type='text/css'> -->

<style>
#attnvis {
  border: 3px solid lightgrey;
  padding: 25px;
  font-family: 'Courier';
  position:relative;
  margin: 10px;
  font-size: 12px;
}
</style>
<script>

json_fname = "/attn_vis/attn_wts_saili.json" // file containing the text and the weights

bluehue = 217
yellowhue = 56.4
redhue = 5
greenhue = 151

function round(x, dp) {
  // round a float to dp decimal places
  var power_of_10 = Math.pow(10,dp)
  return parseFloat(Math.round(x*power_of_10)/power_of_10).toFixed(dp)
}

function toColor(p, hue) {
  // converts a scalar value p in [0,1] to a HSL color code string with base color hue
  if (p<0 || p>1) {
    throw sprintf("Error: p has value %.2f but should be in [0,1]", p)
  }
  var saturation = 100 // saturation percentage
  p = 1-p // invert so p=0 is light and p=1 is dark
  var min_lightness = 50 // minimum percentage lightness, i.e. darkest possible color
  var lightness = (min_lightness + p*(100-min_lightness)) // lightness is proportional to p
  return sprintf('hsl(%d,%s%%,%s%%)', hue, saturation, lightness)
}

function render_srctxt(div, attn_wts) {
  // Render the article in given div. If attn_wts is not null, it is a vector of weights same length as number of article words (or less if truncated); we highlight the article accordingly.
  var article_lst = gdata.article_lst
  var startix = 0;
  var endix = article_lst.length

  div.html(''); // flush
  for(var i=startix; i<endix; i++) {
    var word = article_lst[i]; // a string
    var css = 'display:inline;'
    if (attn_wts != null) {
      var attn_wt = attn_wts[i];
      var background_color = toColor(attn_wt, yellowhue);
      css += 'background-color:' + background_color + ";";
    }
    if (word.slice(0,2)=="__" && word.slice(-2)=="__"){
      word = word.slice(2,-2);
      css += 'font-style: italic;'
    }
    var word_html = word + ' '

    // write the word
    var dnew = div.append('div');
    dnew.attr('class', 'd')
      .attr('style', css) // apply this style
      .html(word_html)
  }
}


function render_summary(div, summary_lst, attn_wts, gen_probs) {
  // Render the summary in the given div.
  // summary_lst is list of words
  // attn_wts and gen_probs are optional
  var startix = 0;
  var endix = summary_lst.length;

  div.html(''); // flush
  for(var i=startix; i<endix; i++) {
    var word = summary_lst[i]; // a string
    var css = 'display:inline;'
    if (gen_probs==null) {
      var gen_prob = null
    } else {
      var gen_prob = gen_probs[i];
      var background_color = toColor(gen_prob, greenhue);
      css += ('background-color:' + background_color + ";");
    }
    if (word.slice(0,2)=="__" && word.slice(-2)=="__") {
      word = word.slice(2,-2);
      css += 'font-style: italic;'
    }
    if (word=="dutch" || word=="irish" || word=="respective" || word=="prospects" || word=="[UNK]") {
      css += 'color: red;'
    }
    if (word==".") {
      word += "<br>"
    }

    var dnew = div.append('div');
    dnew.html(word+' ') // this is the content
      .attr('class', 'd')
      .attr('style', css) // apply this style

    if (attn_wts!=null) {
      // add interactivity for mouseover decoder words
      dnew.on('mouseover', getHandleMouseOver(attn_wts[i], gen_prob))
        .on('mousemove', handleMouseMove)
        .on('mouseout', handleMouseOut)
    }
  }
}

function getHandleMouseOver(attn_wts, gen_prob) {
  // When you mouseover a decoder word, shows attention distribution on article and optionally, gen_prob tooltip
  if (gen_prob==null) {
    return function() {
      render_srctxt(d3.select('#source_text'), attn_wts);
    }
  } else {
    return function() {
      render_srctxt(d3.select('#source_text'), attn_wts);
      gtooltip.text("p_gen = " + round(gen_prob, 3))
      return gtooltip.style("visibility", "visible");
    }
  }
}

function handleMouseMove() {
  // When you move cursor over a decoder word, tooltip follows cursor
  return gtooltip.style("top", (d3.event.pageY-30)+"px").style("left",(d3.event.pageX+10)+"px");
}

function handleMouseOut() {
  // When you move cursor away from a decoder word, stop showing generation probability tooltip and attention distribution
  render_srctxt(d3.select("#source_text"), null);
  return gtooltip.style("visibility", "hidden");
}

function get_json_and_disp() {
  // Retrieve the json data file and display the data
  console.log("fetching " + json_fname + "...")

  function json_success(data) {
    // Displays the data
    console.log("successfully loaded json file.")
    gdata = data; // store globally
    render_srctxt(d3.select("#source_text"), null);
    render_summary(d3.select("#ref"), data.abstract_lst, null, null);
    render_summary(d3.select("#baseline"), data.decoded_lst_baseline, data.attn_wts_baseline, null);
    render_summary(d3.select("#pgen_nocov"), data.decoded_lst_pgen_nocov, data.attn_wts_pgen_nocov, data.gen_probs_pgen_nocov);
    render_summary(d3.select("#pgen_cov"), data.decoded_lst_pgen_cov, data.attn_wts_pgen_cov, data.gen_probs_pgen_cov);
  }

  function json_fail(d) {
    console.log("failure to load." + json_fname)
  }

  $.getJSON(json_fname, json_success).fail(json_fail);
}

function start() {
  console.log("starting...")
  get_json_and_disp()

  // Define a tooltip that we will use to display generation probability of a decoder word when you hover over it
  var tooltip = d3.select("body")
      .append("div")
      .style("position", "absolute")
      .style("z-index", "10")
      .style("visibility", "hidden")
      .style("background", "white")
      .style("font-size", "12px")
      .style("font-family", "Courier")
      .style("border", "2px solid lightgrey")
      .text("a simple tooltip");
  gtooltip = tooltip // global
}

</script>
</head>

<body onload="start();">
  <div id="attnvis">
    <h3>Source Text</h3>
    <div id="source_text">
      source text goes here
    </div>
    <h3>Reference summary</h3>
    <div id="ref">
      reference summary goes here
    </div>
    <h3>Sequence-to-sequence + attention summary</h3>
    <div id="baseline">
      baseline model summary goes here
    </div>
    <h3>Pointer-generator summary</h3>
    <div id="pgen_nocov">
      pointer-generator model summary goes here
    </div>
    <h3>Pointer-generator model + coverage summary</h3>
    <div id="pgen_cov">
      pointer-generator + coverage model summary goes here
    </div>
  </div>
</body>

<p><br /></p>
<h4 id="observations">Observations:</h4>

<ul>
  <li>The basic sequence-to-sequence system is unable to copy out-of-vocabulary words like <em>Saili</em>, outputting the unknown token <font color="red">[UNK]</font> instead. By contrast the pointer-generator systems have no trouble copying this word.</li>
  <li>Though this story happens in <em>New Zealand</em>, the basic sequence-to-sequence system mistakenly reports that the player is <em><font color="red">Dutch</font></em> and the team <em><font color="red">Irish</font></em> – perhaps reflecting the European bias of the training data. When it produced these words, the network was mostly attending to the names <em>Munster</em> and <em>Francis</em> – it seems the system struggled to copy these correctly.</li>
  <li>For reasons unknown, the phrase <em>a great addition to their backline</em> is replaced with the nonsensical phrase <em>a great addition to their</em> <em><font color="red">respective prospects</font></em> by the basic sequence-to-sequence system. Though the network was attending directly to the word <em>backline</em>, it was not copied correctly.</li>
  <li>The basic pointer-generator summary repeats itself, and we see that it’s attending to the same parts of the source text each time. By contrast the pointer-generator + coverage model contains no repetition, and we can see that though it uses the word <em>Saili</em> twice, the network attends to completely different occurrences of the word each time – evidence of the coverage system in action.</li>
  <li>The <span style="background-color: #16e983">green shading</span> shows that the generation probability tends to be high whenever the network is editing the source text. For example, <script type="math/tex">p_{\text{gen}}</script> is high when the network produces a period to shorten a sentence, and when jumping to another part of the text such as <em><strong>will</strong> move to the province…</em> and <em><strong>was</strong> part of the new zealand under-20 side…</em>.</li>
  <li>For all three systems, the attention distribution is fairly focused: usually looking at just one or two words at a time. Errors tend to occur when the attention is more scattered, indicating that perhaps the network is unsure what to do.</li>
  <li>All three systems attend to <em>Munster</em> and <em>Francis</em> when producing the first word of the summary. In general, the networks tend to seek out names to begin summaries.</li>
</ul>

<h3 id="so-is-abstractive-summarization-solved">So, is abstractive summarization solved?</h3>

<p>Not by a long way!
Though we’ve shown that these improvements help to tame some of the wild behavior of Recurrent Neural Networks, there are still <em>many</em> unsolved problems:</p>

<ul>
  <li>Though our system produces abstractive summaries, the wording is usually quite close to the original text. <strong>Higher-level abstraction</strong> – such as more powerful, compressive paraphrasing – remains unsolved.</li>
  <li>Sometimes the network fails to focus on the <strong>core of the source text</strong>, instead choosing to summarize a less important, secondary piece of information.</li>
  <li>Sometimes the network <strong>incorrectly composes fragments</strong> of the source text – for example reporting that <em>Argentina beat Germany 2-0</em> when in fact the opposite was true.</li>
  <li>Multi-sentence summaries sometimes <strong>fail to make sense a whole</strong>, for example referring to an entity by pronoun (e.g. <em>she</em>) without first introducing it (e.g. <em>German Chancellor Angela Merkel</em>).</li>
</ul>

<p>I believe the most important direction for future research is <em>interpretability</em>.
The attention mechanism, by revealing what the network is “looking at”, shines some precious light into the black box of neural networks, helping us to debug problems like repetition and copying.
To make further advances, we need greater insight into <em>what</em> RNNs are learning from text and <em>how</em> that knowledge is represented.</p>

<p>But that’s a story for another day!
In the meantime, check out <a href="https://arxiv.org/pdf/1704.04368.pdf">the paper</a> for more details on our work.</p>
</section>
</article>

<!-- Post navigation -->


<!-- Disqus -->

<div class="comments">
  <div id="disqus_thread"></div>
<script type="text/javascript">
	var disqus_shortname = 'abigailsee-com';
	(function() {
		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div>


<!-- Muut -->


    </div>
    
<script src="/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">Powered by <a href="http://jekyllrb.com">Jekyll</a> with <a href="https://rohanchandra.github.io/project/type/">Type Theme</a>
</p>
</footer>


  </body>
</html>
